{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('seaborn-ticks')\n",
    "SMALL_SIZE = 13\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 16\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"\")\n",
    "df.head(10)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace({'Not Available':np.nan})\n",
    "\n",
    "for col in list(data.columns):\n",
    "    if ('ft²' in col or 'kBtu' in col or 'Metric Tons CO2e' in col or 'kWh' in col \n",
    "        or 'therms' in col or 'gal' in col or 'Score' in col):\n",
    "        data[col] = data[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)#concat on columns\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns\n",
    "missing_values_table(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = missing_values_table(data)\n",
    "\n",
    "missing_columns = list(missing_df[missing_df['% of Total Values'] > 50].index)\n",
    "print('We will remove %d columns.' % len(missing_columns))\n",
    "\n",
    "# Drop the columns\n",
    "data = data.drop(columns = list(missing_columns))\n",
    "\n",
    "# For older versions of pandas (https://github.com/pandas-dev/pandas/issues/19078)\n",
    "# data = data.drop(list(missing_columns), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram Plot of Site EUI\n",
    "figsize(8, 8)\n",
    "plt.hist(data['Site EUI (kBtu/ft²)'].dropna(), bins = 20, edgecolor = 'black');\n",
    "plt.xlabel('Site EUI'); \n",
    "plt.ylabel('Count'); plt.title('Site EUI Distribution');\n",
    "\n",
    "\n",
    "# Calculate first and third quartile\n",
    "first_quartile = data['Site EUI (kBtu/ft²)'].describe()['25%']\n",
    "third_quartile = data['Site EUI (kBtu/ft²)'].describe()['75%']\n",
    "\n",
    "# Interquartile range\n",
    "iqr = third_quartile - first_quartile\n",
    "\n",
    "# Remove outliers\n",
    "data = data[(data['Site EUI (kBtu/ft²)'] > (first_quartile - 3 * iqr)) &\n",
    "            (data['Site EUI (kBtu/ft²)'] < (third_quartile + 3 * iqr))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all correlations and sort \n",
    "correlations_data = data.corr()['score'].sort_values()\n",
    "\n",
    "# Print the most negative correlations\n",
    "print(correlations_data.head(15), '\\n')\n",
    "\n",
    "# Print the most positive correlations\n",
    "print(correlations_data.tail(15))\n",
    "\n",
    "\n",
    "numeric_subset = data.select_dtypes('number')\n",
    "\n",
    "# Create columns with square root and log of numeric columns\n",
    "for col in numeric_subset.columns:\n",
    "    # Skip the Energy Star Score column\n",
    "    if col == 'score':\n",
    "        next\n",
    "    else:\n",
    "        numeric_subset['sqrt_' + col] = np.sqrt(numeric_subset[col])\n",
    "        numeric_subset['log_' + col] = np.log(numeric_subset[col])\n",
    "\n",
    "# Select the categorical columns\n",
    "categorical_subset = data[['Borough', 'Largest Property Use Type']]\n",
    "\n",
    "# One hot encode\n",
    "categorical_subset = pd.get_dummies(categorical_subset)\n",
    "\n",
    "# Join the two dataframes using concat\n",
    "# Make sure to use axis = 1 to perform a column bind\n",
    "features = pd.concat([numeric_subset, categorical_subset], axis = 1)\n",
    "\n",
    "# Drop buildings without an energy star score\n",
    "features = features.dropna(subset = ['score'])\n",
    "\n",
    "# Find correlations with the score \n",
    "correlations = features.corr()['score'].dropna().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_collinear_features(x, threshold):\n",
    "    '''\n",
    "    Objective:\n",
    "        Remove collinear features in a dataframe with a correlation coefficient\n",
    "        greater than the threshold. Removing collinear features can help a model\n",
    "        to generalize and improves the interpretability of the model.\n",
    "        \n",
    "    Inputs: \n",
    "        threshold: any features with correlations greater than this value are removed\n",
    "    \n",
    "    Output: \n",
    "        dataframe that contains only the non-highly-collinear features\n",
    "    '''\n",
    "    \n",
    "    # Dont want to remove correlations between Energy Star Score\n",
    "    y = x['score']\n",
    "    x = x.drop(columns = ['score'])\n",
    "    \n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = x.corr()\n",
    "    iters = range(len(corr_matrix.columns) - 1)\n",
    "    drop_cols = []\n",
    "\n",
    "    # Iterate through the correlation matrix and compare correlations\n",
    "    for i in iters:\n",
    "        for j in range(i):\n",
    "            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
    "            col = item.columns\n",
    "            row = item.index\n",
    "            val = abs(item.values)\n",
    "            \n",
    "            # If correlation exceeds the threshold\n",
    "            if val >= threshold:\n",
    "                # Print the correlated features and the correlation value\n",
    "                # print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
    "                drop_cols.append(col.values[0])\n",
    "\n",
    "    # Drop one of each pair of correlated columns\n",
    "    drops = set(drop_cols)\n",
    "    x = x.drop(columns = drops)\n",
    "    x = x.drop(columns = ['Weather Normalized Site EUI (kBtu/ft²)', \n",
    "                          'Water Use (All Water Sources) (kgal)',\n",
    "                          'log_Water Use (All Water Sources) (kgal)',\n",
    "                          'Largest Property Use Type - Gross Floor Area (ft²)'])\n",
    "    \n",
    "    # Add the score back in to the data\n",
    "    x['score'] = y\n",
    "               \n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Remove the collinear features above a specified correlation coefficient\n",
    "features = remove_collinear_features(features, 0.6)\n",
    "# Remove any columns with all na values\n",
    "features  = features.dropna(axis=1, how = 'all')\n",
    "features.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Separate out the features and targets\n",
    "features = score.drop(columns='score')\n",
    "targets = pd.DataFrame(score['score'])\n",
    "\n",
    "# Replace the inf and -inf with nan (required for later imputation)\n",
    "features = features.replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "\n",
    "# Split into 70% training and 30% testing set\n",
    "X, X_test, y, y_test = train_test_split(features, targets, test_size = 0.3, random_state = 42)\n",
    "\n",
    "print(X.shape)\n",
    "print(X_test.shape)\n",
    "print(y.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Groupby_OneCol_comp_plot(df, col, plt_style = 'seaborn-ticks',color_palette = 'coolwarm'):\n",
    "    gr = pd.DataFrame()\n",
    "    gr['{} No'.format(col)] = df.groupby(col).size()\n",
    "    gr['{} Ratio'.format(col)] = np.round(gr['{} No'.format(col)].divide(gr['{} No'.format(col)].sum())*100,0)\n",
    "    print('Total No. of {} {}'.format(col,gr['{} No'.format(col)].sum()))\n",
    "    plt.style.use(plt_style)\n",
    "    sns.set_palette(sns.color_palette(color_palette))\n",
    "    fig = plt.figure()\n",
    "    plt.axis('off')\n",
    "    fig.add_subplot(121)\n",
    "    \n",
    "    ax = gr['{} No'.format(col)].plot(kind='bar',title='{} Counts'.format(col), figsize=(16,8), color=sns.color_palette())\n",
    "    _ = plt.setp(ax.get_xticklabels(), rotation = 0)\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(np.round(p.get_height(),decimals=2),(p.get_x()+p.get_width()/2.,p.get_height()), ha='center', \n",
    "                    va='center', xytext=(0,10), textcoords='offset points')\n",
    "    ax.get_yaxis().set_ticks([])\n",
    "    plt.xlabel('')\n",
    "\n",
    "    fig.add_subplot(122)\n",
    "    plt.axis('off')\n",
    "    gr.loc[:,'{} Ratio'.format(col)].plot(kind='pie', \n",
    "                                          autopct='%1.1f%%',shadow=False,title='{} Ratio'.format(col),\n",
    "                                          legend=False,labels=None);\n",
    "    sns.despine(top=True, right=True, left=True, bottom = False);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(table, legloc='upper right', plt_style= 'seabotn-ticks', color_palette='dark',sorter=None, stacked=False,\n",
    "        kind ='bar', percentage=True, custom_title=None, minimal=True,figsize=(19,10),width=0.7):\n",
    "    grouped = table\n",
    "    if percentage == True:\n",
    "        grouped = np.round(grouped.divide(grouped['Total'],axis=0)*100,0)\n",
    "    try:\n",
    "        del grouped['Total']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if sorter:\n",
    "        grouped = grouped[sorter]\n",
    "        \n",
    "    plt.style.use(plt_style)\n",
    "    sns.set_palette(sns.color_palette(color_palette))\n",
    "    ax = grouped.plot(kind=kind, stacked=stacked, figsize=figsize, width=width)\n",
    "    _ = plt.setp(ax.get_xticklabels(), rotation=0)\n",
    "    plt.legend(loc = legloc)\n",
    "    \n",
    "    if percentage == True:\n",
    "        for p in ax.patches:\n",
    "            ax.annotate('{}%'.format(int(np.round(p.get_height(),decimals=2))),\n",
    "                       (p.get_x()+p.get_width()/2. , p.get_height()), ha='center',\n",
    "                        va = 'center', xytext=(0,10), textcoords = 'offset points')\n",
    "    else:\n",
    "        for p in ax.patches():\n",
    "            ax.annotate(np.round(p.get_height(), decimals=2), (p.get_x()+p.get_width()/2.,p.get_height()), \n",
    "            ha='center', va='center', xytext=(0,10), textcoords='offset points')\n",
    "    \n",
    "    if minimal == True:\n",
    "        ax.get_yaxis().set_ticks([])\n",
    "        plt.xlabel('')\n",
    "        sns.despine(top=True, right=True, left= True, bottom=False);\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    plt.title(custom_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Groupby_TwoCol_Plot(df, col1, col2, legloc = 'upper right', plt_style = 'ggplot', \n",
    "                        color_palette = 'dark', sorter = None, stacked=False, kind='bar',\n",
    "                       percentage=True, custom_title=None, minimal=True, figsize=(14,6), width=0.6):\n",
    "    \n",
    "    grouped = df.groupby([col2, col1]).size().unstack(col2)\n",
    "    grouped['Total'] = grouped.sum(axis=1)\n",
    "    #grouped = grouped.sort_values('Total', ascending = False)\n",
    "    plot(grouped, legloc=legloc, plt_style = plt_style, color_palette = color_palette, sorter=sorter,\n",
    "        stacked = stacked, kind = kind, percentage = percentage, custom_title = custom_title, minimal= minimal, \n",
    "         figsize = figsize, width = width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_labels = ['0-9', '10-19', '20-29', '30-39', '40-49', '50-59', '60-69', '70-80']\n",
    "age_group_values = pd.cut(not_missing.Age, range(0,81,10), right=True, labels=age_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "deck = {\"A\":1, \"B\":2, \"C\":3, \"D\":4, \"E\":5, \"F\":6, \"G\":7, \"U\":8}\n",
    "data = [train_df, test_df]\n",
    "\n",
    "for dataset in data:\n",
    "    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n",
    "    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "    dataset['Deck'] = dataset['Deck'].map(deck)\n",
    "    dataset['Deck'] = dataset['Deck'].fillna(0)\n",
    "    dataset['Deck'] = dataset['Deck'].astype(int)\n",
    "\n",
    "train_df = train_df.drop(['Cabin'], axis=1)\n",
    "test_df = test_df.drop(['Cabin'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = [train_df, test_df]\n",
    "for dataset in data:\n",
    "    med = train_df['Age'].median()\n",
    "    std = test_df['Age'].std()\n",
    "    is_null = dataset['Age'].isnull().sum()\n",
    "    rand_age = np.random.randint(med-std, med+std, size = is_null)\n",
    "    age_slice = dataset['Age'].copy()#array of values\n",
    "    age_slice[np.isnan(age_slice)] = rand_age\n",
    "    dataset['Age'] = age_slice\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "    \n",
    "train_df['Age'].isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_df['Embarked'].describe()\n",
    "\n",
    "com_val = 'S'\n",
    "data = [train_df, test_df]\n",
    "for dataset in data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna(com_val)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "train_df.info()\n",
    "\n",
    "\n",
    "data = [train_df, test_df]\n",
    "\n",
    "for dataset in data:\n",
    "\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(0)\n",
    "\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "\n",
    "data = [train_df, test_df]\n",
    "titles = {'Mr':1,'Miss':2,'Mrs':3,'Master':4,'Rare':5}\n",
    "\n",
    "for dataset in data:\n",
    "    dataset['Title'] = dataset.Name.str.extract('([A-Za-z]+)\\.',expand = False)\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady','Countess','Capt','Col','Don','Dr','Major','Rev',\n",
    "                                                'Sir','Jonkheer','Dona'],'Rare')\n",
    "    dataset['Title'] = dataset['Title'].replace(['Mlle','Ms'],'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme','Mrs')\n",
    "\n",
    "title = pd.get_dummies(train_df['Title'])\n",
    "train_df = pd.concat([train_df,title],axis=1)\n",
    "\n",
    "title = pd.get_dummies(test_df['Title'])\n",
    "test_df = pd.concat([test_df,title],axis=1)\n",
    "#dataset['Title'] = dataset['Title'].map(titles)\n",
    "#dataset['Title'] = dataset['Title'].fillna(0)\n",
    "#dataset = dataset.drop(['Name'], axis=1)\n",
    "\n",
    "train_df = train_df.drop(['Name','Title'],axis=1)\n",
    "test_df = test_df.drop(['Name','Title'],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = [train_df, test_df]\n",
    "\n",
    "for dataset in data:\n",
    "\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "\n",
    "    dataset.loc[dataset['Age'] <=11, 'Age'] = 0\n",
    "\n",
    "    dataset.loc[(dataset['Age'] >11) & (dataset['Age'] <=18), 'Age'] = 1\n",
    "\n",
    "    dataset.loc[(dataset['Age'] >18) & (dataset['Age'] <=22), 'Age'] = 2\n",
    "\n",
    "    dataset.loc[(dataset['Age'] >22) & (dataset['Age'] <=27), 'Age'] = 3\n",
    "\n",
    "    dataset.loc[(dataset['Age'] >27) & (dataset['Age'] <=33), 'Age'] = 4\n",
    "\n",
    "    dataset.loc[(dataset['Age'] >33) & (dataset['Age'] <=40), 'Age'] = 5\n",
    "\n",
    "    dataset.loc[dataset['Age'] >40, 'Age'] = 6\n",
    "\n",
    "train_df['Age'].value_counts()\n",
    "data = [train_df, test_df]\n",
    "for dataset in data:\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "    dataset.loc[dataset['Fare'] <=7.91, 'Fare'] = 0\n",
    "    dataset.loc[(dataset['Fare'] >7.91) & (dataset['Fare'] <=14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] >14.454) & (dataset['Fare'] <=31), 'Fare'] = 2\n",
    "    dataset.loc[(dataset['Fare'] >31) & (dataset['Fare'] <=99), 'Fare'] = 3\n",
    "    dataset.loc[(dataset['Fare'] >99) & (dataset['Fare'] <=250), 'Fare'] = 4\n",
    "    #dataset.loc[(dataset['Fare'] >33) & (dataset['Fare'] <=40), 'Fare'] = 5\n",
    "    dataset.loc[dataset['Fare'] >250, 'Fare'] = 5\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "train_df['Fare'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "scores = cross_val_score(rf, X_train, Y_train, cv=10, scoring='accuracy')\n",
    "\n",
    "print(\"Scores:\", scores)\n",
    "print(\"Mean:\", scores.mean())\n",
    "print(\"Standard Deviation:\",scores.std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
